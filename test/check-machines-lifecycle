#!/usr/bin/python3

# This file is part of Cockpit.
#
# Copyright (C) 2021 Red Hat, Inc.
#
# Cockpit is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or
# (at your option) any later version.
#
# Cockpit is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Cockpit; If not, see <http://www.gnu.org/licenses/>.

import os
import sys

# import Cockpit's machinery for test VMs and its browser test API
TEST_DIR = os.path.dirname(__file__)
sys.path.append(os.path.join(TEST_DIR, "common"))
sys.path.append(os.path.join(os.path.dirname(TEST_DIR), "bots/machine"))

from machineslib import VirtualMachinesCase  # noqa
from testlib import nondestructive, skipImage, test_main, wait  # noqa

distrosWithoutSnapshot = ["centos-8-stream", "rhel-8-6", "rhel-8-7"]


@nondestructive
class TestMachinesLifecycle(VirtualMachinesCase):

    def testBasic(self):
        self._testBasic()

    def createUser(self, user_group):
        user_name = f"test_{user_group if user_group else 'none'}_user"
        self.machine.execute(f"useradd{' -G ' + user_group if user_group else '' } {user_name} && echo '{user_name}:foobar' | chpasswd")
        # user libvirtd instance tends to SIGABRT with "Failed to find user record for uid .." on shutdown during cleanup
        # so make sure that there are no leftover user processes that bleed into the next test
        self.addCleanup(self.machine.execute, "pkill -u {0}; while pgrep -u {0}; do sleep 0.5; done".format(user_name))
        # HACK: ...but it still tends to crash during shutdown (without known stack trace)
        self.allow_journal_messages('Process .*libvirtd.* of user 10.* dumped core.*')

        return user_name

    def testBasicNonrootUserUnprivileged(self):
        user = self.createUser(user_group=None)
        self._testBasic(user, False, True)

    # FIXME remove this skipImage
    @skipImage('Fails with Rejected send message, 1 matched rules; type="method_call"',
               "rhel-8-6", "ubuntu-stable", "debian-testing", "debian-stable", "centos-8-stream")
    def testBasicLibvirtUserUnprivileged(self):
        user = self.createUser(user_group='libvirt')
        self._testBasic(user, False)

    def testBasicAdminUser(self):
        # The group in debian based distro should be sudo
        if "ubuntu" in self.machine.image or "debian" in self.machine.image:
            user_group = "sudo"
        else:
            user_group = "wheel"
        user = self.createUser(user_group=user_group)

        # Check NON-PRIVILEGED user won't see any VMs even though they are in wheel group
        self._testBasic(user, False, True)
        self.machine.execute("virsh destroy subVmTest1 && virsh undefine subVmTest1")
        # Check wheel group PRIVILEGED user will see VMs and can do operations on them
        self._testBasic(user, True, False)

    def _testBasic(self, user=None, superuser=True, expect_empty_list=False):
        b = self.browser
        m = self.machine

        args = self.createVm("subVmTest1")

        self.login_and_go("/machines", user=user, superuser=superuser)

        if expect_empty_list:
            with b.wait_timeout(20):
                b.wait_in_text("#virtual-machines-listing .pf-c-empty-state", "No VM is running")
                return
        b.wait_in_text("body", "Virtual machines")
        self.waitVmRow("subVmTest1")

        b.wait_in_text("#vm-subVmTest1-system-state", "Running")
        self.goToVmPage("subVmTest1")
        b.wait_in_text("#vm-subVmTest1-vcpus-count", "1")

        b.wait_in_text("#vm-subVmTest1-boot-order", "disk,network")
        emulated_machine = b.text("#vm-subVmTest1-emulated-machine")
        self.assertTrue(len(emulated_machine) > 0)  # emulated machine varies across test machines

        # switch to and check Usage
        b.click("#vm-subVmTest1-usage")
        b.wait_in_text(".memory-usage-chart .pf-c-progress__status > .pf-c-progress__measure", "128 MiB")
        b.wait_not_in_text(".memory-usage-chart .pf-c-progress__status > .pf-c-progress__measure", "0 /")
        usage = b.text(".memory-usage-chart .pf-c-progress__status > .pf-c-progress__measure").split("/ 128 MiB")[0]
        wait(lambda: float(usage) > 0.0, delay=3)

        b.wait_in_text(".vcpu-usage-chart .pf-c-progress__status > .pf-c-progress__measure", "1 vCPU")
        usage = b.text(".vcpu-usage-chart .pf-c-progress__status > .pf-c-progress__measure").split("% of 1 vCPU")[0]
        # CPU usage cannot be nonzero with blank image, so just ensure it's a percentage
        wait(lambda: float(usage) <= 100.0, delay=3)

        # suspend/resume
        m.execute("virsh suspend subVmTest1")
        b.wait_in_text("#vm-subVmTest1-system-state", "Paused")
        # resume sometimes fails with "unable to execute QEMU command 'cont': Resetting the Virtual Machine is required"
        m.execute('virsh resume subVmTest1 || { virsh destroy subVmTest1 && virsh start subVmTest1; }')
        b.wait_in_text("#vm-subVmTest1-system-state", "Running")

        # Wait for the system to completely start
        wait(lambda: "login as 'cirros' user." in self.machine.execute(f"cat {args['logfile']}"), delay=3)

        # Send Non-Maskable Interrupt (no change in VM state is expected)
        self.performAction("subVmTest1", "sendNMI")

        b.wait(lambda: "NMI received" in self.machine.execute(f"cat {args['logfile']}"))

        # pause
        self.performAction("subVmTest1", "pause")
        # check removing button of disk and network interface when the VM is paused
        b.wait_visible("#delete-subVmTest1-disk-vda:disabled")
        b.wait_visible("#delete-vm-subVmTest1-iface-1:disabled")

        # resume
        self.performAction("subVmTest1", "resume")

        # reboot
        self.machine.execute(f"echo '' > {args['logfile']}")
        self.performAction("subVmTest1", "reboot")
        wait(lambda: "reboot: Power down" in self.machine.execute(f"cat {args['logfile']}"), delay=3)
        b.wait_in_text("#vm-subVmTest1-system-state", "Running")

        # force reboot
        self.machine.execute(f"echo '' > {args['logfile']}")
        self.performAction("subVmTest1", "forceReboot")
        wait(lambda: "Initializing cgroup subsys" in self.machine.execute(f"cat {args['logfile']}"), delay=3)
        b.wait_in_text("#vm-subVmTest1-system-state", "Running")

        # shut off
        self.performAction("subVmTest1", "forceOff")

        # continue shut off validation - usage should drop to zero
        b.wait_in_text(".memory-usage-chart .pf-c-progress__status > .pf-c-progress__measure", "0 /")
        b.wait_in_text(".vcpu-usage-chart .pf-c-progress__status > .pf-c-progress__measure", "0%")

        # shut off of a transient VM will redirect us to the main page
        m.execute("virsh dumpxml subVmTest1 > /tmp/subVmTest1.xml")
        m.execute("virsh start {0} && virsh undefine {0}".format("subVmTest1"))
        b.wait_visible("div[data-vm-transient=\"true\"]")
        self.performAction("subVmTest1", "forceOff", checkExpectedState=False)
        b.wait_in_text("#virtual-machines-listing .pf-c-empty-state", "No VM is running")
        m.execute("virsh define --file /tmp/subVmTest1.xml")

        # start another one, should appear automatically
        self.createVm("subVmTest2")
        b.wait_in_text("#vm-subVmTest2-system-state", "Running")
        self.goToVmPage("subVmTest2")
        b.wait_in_text("#vm-subVmTest2-vcpus-count", "1")
        b.wait_in_text("#vm-subVmTest2-boot-order", "disk,network")

        self.goToMainPage()
        self.waitVmRow("subVmTest1")
        self.waitVmRow("subVmTest2")

        # stop second VM, event handling should still work
        self.performAction("subVmTest2", "forceOff")

        b.click("#vm-subVmTest2-system-run")

        b.set_input_text("#text-search", "subVmTest2")
        self.waitVmRow("subVmTest2")
        self.waitVmRow("subVmTest1", "system", False)

        b.select_PF4("#vm-state-select-toggle", "Running")
        self.waitVmRow("subVmTest1", "system", False)
        self.waitVmRow("subVmTest2")

        b.set_input_text("#text-search", "")
        self.waitVmRow("subVmTest1", "system", False)
        self.waitVmRow("subVmTest2")

        b.select_PF4("#vm-state-select-toggle", "Shut off")
        self.waitVmRow("subVmTest1")
        self.waitVmRow("subVmTest2", "system", False)

        b.select_PF4("#vm-state-select-toggle", "All")
        self.waitVmRow("subVmTest1")
        self.waitVmRow("subVmTest2")

        # Check correctness of the toast notifications list
        # We 'll create errors by starting to start domains when the default network in inactive
        self.createVm("subVmTest3")
        m.execute("virsh destroy subVmTest2 && virsh destroy subVmTest3 && virsh net-destroy default")

        def tryRunDomain(index, name, connection):
            self.waitVmRow(name)

            b.click(f"#vm-{name}-{connection}-run")

        # Try to run subVmTest1 - it will fail because of inactive default network
        tryRunDomain(1, 'subVmTest1', 'system')
        b.click('#vm-subVmTest1-system-state button:contains("view more")')
        b.wait_in_text(".pf-c-popover", "VM subVmTest1 failed to start")
        b.click('#vm-subVmTest1-system-state button[aria-label=Close]')

        # Try to run subVmTest2
        tryRunDomain(2, 'subVmTest2', 'system')
        b.click('#vm-subVmTest2-system-state button:contains("view more")')
        b.wait_in_text(".pf-c-popover", "VM subVmTest2 failed to start")
        b.click('#vm-subVmTest2-system-state button[aria-label=Close]')

    def testCloneSessionConnection(self):
        self.testClone(connectionName='session')

    def testClone(self, connectionName='system'):
        b = self.browser

        self.run_admin("mkdir /tmp/vmdir", connectionName)
        self.addCleanup(self.run_admin, "rm -rf /tmp/vmdir/", connectionName)

        self.login_and_go("/machines")
        b.wait_in_text("body", "Virtual machines")

        self.createVm("subVmTest1", running=False, connection=connectionName)

        self.waitVmRow("subVmTest1", connectionName=connectionName)

        self.performAction("subVmTest1", "clone", connectionName=connectionName)

        b.wait_text(".pf-c-modal-box__title-text", "Create a clone VM based on subVmTest1")
        b.click("footer button.pf-m-primary")
        b.wait_not_present(".pf-c-modal-box")
        self.waitVmRow("subVmTest1-clone", connectionName=connectionName)

    def testRename(self):
        b = self.browser

        self.createVm("old", running=False)

        self.login_and_go("/machines")

        b.wait_in_text("body", "Virtual machines")

        self.performAction("old", "rename")

        b.set_input_text("#rename-dialog-new-name", "new")
        b.click("#rename-dialog-confirm")
        self.waitVmRow("new")
        self.waitVmRow("old", "system", False)

        # Rename to itself and expect error
        self.performAction("new", "rename")

        b.set_input_text("#rename-dialog-new-name", "new")
        b.click("#rename-dialog-confirm")
        b.wait_in_text(".pf-c-modal-box__footer .pf-c-alert.pf-m-danger", "Can't rename domain to itself")

        self.goToVmPage("new")
        self.performAction("new", "rename")

        b.set_input_text("#rename-dialog-new-name", "test%")
        b.click("#rename-dialog-confirm")
        b.wait(lambda: "vm?name=test%25&connection=system" in b.eval_js("window.location.href"))
        b.wait_text("h2.vm-name", "test%")
        b.wait_not_present("#navbar-oops")

    def testLibvirt(self):
        b = self.browser
        m = self.machine

        self.allow_restart_journal_messages()

        libvirtServiceName = self.getLibvirtServiceName()

        # disable journal core dumps and traces for this test -- libvirt tends to crash on stopping
        core_pattern = m.execute("cat /proc/sys/kernel/core_pattern").strip()
        m.execute("echo core > /proc/sys/kernel/core_pattern")
        self.addCleanup(m.execute, f"echo '{core_pattern}' > /proc/sys/kernel/core_pattern")

        def stopLibvirtSocket():
            m.execute(f"systemctl stop {self.getLibvirtServiceName()}-ro.socket {self.getLibvirtServiceName()}.socket {self.getLibvirtServiceName()}-admin.socket")
            self.addCleanup(m.execute, f"systemctl start {self.getLibvirtServiceName()}-ro.socket {self.getLibvirtServiceName()}.socket {self.getLibvirtServiceName()}-admin.socket")

        def stopLibvirtService():
            m.execute(f"systemctl stop {libvirtServiceName}")

        def startLibvirtSocket():
            m.execute(f"systemctl start {libvirtServiceName}")

        def hack_libvirtd_crash():
            # work around libvirtd crashing when stopped too quickly; https://bugzilla.redhat.com/show_bug.cgi?id=1828207
            m.execute("virsh domifaddr 1")

        def waitEmptyState():
            with b.wait_timeout(15):
                b.wait_in_text(".pf-c-empty-state", "Virtualization service (libvirt) is not active")

        # Check initial state
        self.createVm("subVmTest1")
        self.login_and_go("/machines")
        b.wait_in_text("body", "Virtual machines")
        self.waitVmRow("subVmTest1")

        # Check that empty state screen appears for privileged users when both the service and the socket are not running
        hack_libvirtd_crash()
        stopLibvirtSocket()
        stopLibvirtService()
        b.reload()
        b.enter_page('/machines')
        waitEmptyState()
        b.click(".pf-c-empty-state button.pf-m-link")  # Troubleshoot
        b.leave_page()
        b.wait(lambda: "system/services" in b.eval_js("window.location.href"))

        # Make sure that unprivileged users can see the VM list when libvirtd is not running
        m.execute("useradd nonadmin; echo nonadmin:foobar | chpasswd")
        # user libvirtd instance tends to SIGABRT with "Failed to find user record for uid .." on shutdown during cleanup
        # so make sure that there are no leftover user processes that bleed into the next test
        self.addCleanup(self.machine.execute, "pkill -u nonadmin; while pgrep -u nonadmin; do sleep 0.5; done")
        self.login_and_go("/machines", user="nonadmin", superuser=False)
        b.wait_in_text("body", "Virtual machines")
        b.wait_in_text("#virtual-machines-listing .pf-c-empty-state", "No VM is running")

    def testDelete(self):
        b = self.browser
        m = self.machine

        name = "subVmTest1"
        img2 = f"/var/lib/libvirt/images/{name}-2.img"

        args = self.createVm(name, graphics='vnc')

        self.login_and_go("/machines")
        b.wait_in_text("body", "Virtual machines")
        self.waitVmRow(name)

        m.execute(f"test -f {img2}")

        self.goToVmPage("subVmTest1")

        def addDisk(volName, poolName):
            # Virsh does not offer some option to create disks of type volume
            # We have to do this from cockpit UI
            b.click("#vm-subVmTest1-disks-adddisk")  # button
            b.wait_visible("#vm-subVmTest1-disks-adddisk-dialog-modal-window")
            b.wait_visible("label:contains(Create new)")  # radio button label in the modal dialog

            b.select_from_dropdown("#vm-subVmTest1-disks-adddisk-new-select-pool", poolName)
            b.set_input_text("#vm-subVmTest1-disks-adddisk-new-name", volName)
            b.set_input_text("#vm-subVmTest1-disks-adddisk-new-size", "10")
            b.select_from_dropdown("#vm-subVmTest1-disks-adddisk-new-unit", "MiB")
            b.click("#vm-subVmTest1-disks-adddisk-permanent")

            b.click("#vm-subVmTest1-disks-adddisk-dialog-add")
            b.wait_not_present("#vm-subVmTest1-disks-adddisk-dialog-modal-window")

            b.wait_visible("#vm-subVmTest1-disks-vdb-source-volume")
            b.wait_visible("#vm-subVmTest1-disks-vdb-source-pool")

        secondDiskVolName = "mydisk"
        poolName = "images"
        secondDiskPoolPath = "/var/lib/libvirt/images/"

        addDisk(secondDiskVolName, poolName)

        self.performAction(name, "delete")

        b.wait_visible(f"#vm-{name}-delete-modal-dialog .pf-c-modal-box__body:contains(The VM {name} is running)")
        b.wait_visible(f"#vm-{name}-delete-modal-dialog ul li:first-child .disk-source-file:contains({img2})")
        # virsh attach-disk does not create disks of type volume
        b.wait_visible(f"#vm-{name}-delete-modal-dialog .disk-source-volume:contains({secondDiskVolName})")
        b.wait_visible(f"#vm-{name}-delete-modal-dialog .disk-source-pool:contains({poolName})")
        b.assert_pixels(f"#vm-{name}-delete-modal-dialog", "vm-delete-dialog")
        b.click(f"#vm-{name}-delete-modal-dialog button:contains(Delete)")
        b.wait_not_present(f"#vm-{name}-delete-modal-dialog")

        self.waitVmRow(name, "system", False)

        m.execute(f"while test -f {img2}; do sleep 1; done")
        m.execute(f"while test -f {secondDiskPoolPath + secondDiskVolName}; do sleep 1; done")

        self.assertNotIn(name, m.execute("virsh list --all --name"))

        # Try to delete a paused VM
        name = "paused-test-vm"
        args = self.createVm(name)

        self.goToVmPage(name)

        # Make sure that the VM booted normally before attempting to suspend it
        wait(lambda: "login as 'cirros' user" in self.machine.execute(f"cat {args['logfile']}"), delay=3)

        self.machine.execute(f"virsh -c qemu:///system suspend {name}")
        b.wait_in_text(f"#vm-{name}-system-state", "Paused")
        self.performAction(name, "delete")
        b.click(f"#vm-{name}-delete-modal-dialog button:contains(Delete)")
        self.waitVmRow(name, 'system', False)

        if m.image not in distrosWithoutSnapshot:
            # Delete a VM with snapshots and ensure the qcow2 image overlays get also cleaned up
            name = "vm-with-snapshots"
            self.createVm(name, running=False)

            m.execute(f"virsh snapshot-create-as --domain {name} --name snapshotA --description 'Description of snapshotA'")

            qemu_img_info = m.execute(f"qemu-img info /var/lib/libvirt/images/{name}-2.img")
            self.assertIn("snapshotA", qemu_img_info)

            # snapshots events not available yet: https://gitlab.com/libvirt/libvirt/-/issues/44
            b.reload()
            b.enter_page('/machines')
            self.goToVmPage(name)
            b.wait_in_text(f"#vm-{name}-snapshot-0-name", "snapshotA")

            self.performAction(name, "delete")

            # Unselect the disks - we don't want to delete them
            b.click("#vm-vm-with-snapshots-delete-modal-dialog ul li:first-child input")
            b.click("#vm-vm-with-snapshots-delete-modal-dialog button:contains(Delete)")
            b.wait_not_present(f"#vm-{name}-delete-modal-dialog")

            qemu_img_info = m.execute(f"qemu-img info /var/lib/libvirt/images/{name}-2.img")
            self.assertNotIn("snapshotA", qemu_img_info)

        # Try to delete a VM and keep its volume
        name = "vm-keep-vol"
        args = self.createVm(name)
        self.waitVmRow(name)
        wait(lambda: name in m.execute("virsh list --all --name"))

        self.performAction(name, "delete")
        b.set_checked(f"#vm-{name}-delete-modal-dialog input[type=checkbox]", False)
        b.click(f"#vm-{name}-delete-modal-dialog button:contains(Delete)")

        self.waitVmRow(name, present=False)
        wait(lambda: name not in m.execute("virsh list --all --name"))
        m.execute(f"test -f {args['image']}")

        # Try to delete a transient VM
        name = "transient-VM"
        args = self.createVm(name)
        m.execute(f"virsh undefine {name}")
        b.wait_visible(f"tr[data-row-id=vm-{name}-system][data-vm-transient=true]")
        b.click(f"#vm-{name}-system-action-kebab button")
        b.wait_visible(f"#vm-{name}-system-delete a.pf-m-disabled")
        b.click(f"#vm-{name}-system-forceOff")
        self.waitVmRow(name, 'system', False)
        b.wait_not_present(f'#vm-{name}-system-state button:contains("view more")')

        # Try to delete a VM where storage deletion will fail
        name = "vm-fail-storage-deletion"
        args = self.createVm(name)
        self.waitVmRow(name)
        wait(lambda: name in m.execute("virsh list --all --name"))
        # Remove VM's disk from command line
        m.execute(f"rm {args['image']}")

        self.performAction(name, "delete")
        b.set_checked(f"#vm-{name}-delete-modal-dialog input[type=checkbox]", True)
        b.click(f"#vm-{name}-delete-modal-dialog button:contains(Delete)")

        # Check VM got deleted, but there is a warning about unsuccessful storage deletion
        self.waitVmRow(name, present=False)
        wait(lambda: name not in m.execute("virsh list --all --name"))
        b.wait_visible(".pf-c-alert-group li .pf-c-alert")
        b.wait_in_text(".pf-c-alert-group li .pf-c-alert .pf-c-alert__title", f"Could not delete storage for {name}")
        b.click("button.alert-link.more-button")
        b.wait_in_text(".pf-c-alert-group li .pf-c-alert .pf-c-alert__description", args['image'])
        # Close the notification
        b.click(".pf-c-alert-group li .pf-c-alert button.pf-m-plain")

        # Delete a shut-off guest and verify the storage was removed
        name = "vm-shutoff"
        img2 = f"/var/lib/libvirt/images/{name}-2.img"
        self.createVm(name, running=False)
        # Guest disk image exists
        m.execute(f"while ! test -f {img2}; do sleep 1; done")

        self.performAction(name, "delete")

        b.wait_visible(f"#vm-{name}-delete-modal-dialog .disk-source-file:contains({img2})")
        b.wait_visible(f"#vm-{name}-delete-modal-dialog input[name='check-action-vda']:checked")
        b.click(f"#vm-{name}-delete-modal-dialog button:contains(Delete)")
        b.wait_not_present(f"#vm-{name}-delete-modal-dialog")

        self.waitVmRow(name, "system", False)

        # Guest disk image has been deleted
        m.execute(f"while test -f {img2}; do sleep 1; done")

        self.assertNotIn(name, m.execute("virsh list --all --name"))

        # Deleting a running guest will disconnect the serial console
        self.allow_browser_errors("Disconnection timed out.")
        self.allow_journal_messages(".* couldn't shutdown fd: Transport endpoint is not connected")


if __name__ == '__main__':
    test_main()
